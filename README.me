Sight & Sound: Image Analysis & Text-to-Speech Web AppSight & Sound is a web application that demonstrates how to integrate Azure AI services to perform advanced image analysis and convert the resulting descriptions into spoken audio. The app uses an Azure Function as a backend to connect the web interface to the Azure AI Vision and Azure AI Speech APIs.‚ú® FeaturesImage Input Flexibility: Analyze images from a URL, by uploading a file, or by taking a snapshot from your webcam.Deep Image Analysis: Utilizes Azure AI Vision to extract a comprehensive set of visual features:Caption: A single, concise sentence describing the main content of the image.Dense Captions: Multiple, detailed descriptions of specific regions within the image.Tags: A list of keywords and concepts found in the image.Objects: Detection and identification of physical objects.OCR (Read): Extraction of printed and handwritten text.Dynamic Text-to-Speech (TTS): Converts the image's main caption into natural-sounding speech using Azure AI Speech.Auto-Speak Feature: An optional setting to automatically play the caption audio immediately after a successful analysis.Simple Configuration: Easily set and save the API base URL directly within the app interface.üöÄ Getting StartedThis guide will walk you through setting up the required Azure resources and deploying the application.PrerequisitesAn active Azure subscription.Node.js (LTS version).Azure CLI or Azure PowerShell.Step 1: Create Azure ResourcesYou will need an Azure AI Services multi-service account and an Azure Function App to host the backend logic.Create the Azure AI Services Account:In the Azure Portal, search for "Azure AI services" and create a new multi-service resource.Choose a unique name, select your subscription, resource group, and a supported region.After creation, go to the resource's "Keys and Endpoint" page and copy the Endpoint URL and one of the Keys. You will need these in the next step.Create the Azure Function App:In the Azure Portal, search for "Function App" and create a new one.Choose a unique name and select the same region as your AI Services account.For the runtime stack, select Node.js and the latest LTS version (e.g., 20 LTS).Create the function app.Step 2: Configure Environment VariablesThe Azure Function needs to know your AI services endpoint and key to make API calls. You will set these as Application Settings in your Function App.In the Azure Portal, navigate to your new Function App.Under Settings, select Configuration.Add the following new application settings:VISION_ENDPOINT: The Endpoint URL you copied from your Azure AI Services resource.VISION_KEY: The API key you copied.SPEECH_REGION: The region where your Azure AI Services resource is located (e.g., westus2).SPEECH_KEY: The same API key as your VISION_KEY.Step 3: Deploy the CodeThis project contains both the frontend (HTML/CSS/JS) and the backend (Azure Function) code. You'll deploy them together to your Azure Function App.Clone this repository to your local machine.Open a terminal in the project directory.Use the Azure Functions Core Tools to deploy: func azure functionapp publish <YOUR_FUNCTION_APP_NAME>.Step 4: Use the AppOnce deployed, you can access the app's URL (e.g., https://<YOUR_FUNCTION_APP_NAME>.azurewebsites.net). The web interface will allow you to choose an image and perform the analysis.üõ†Ô∏è Tech StackFrontend: HTML, CSS, JavaScriptBackend: Azure Functions (Node.js)AI Services: Azure AI Vision, Azure AI Speech
